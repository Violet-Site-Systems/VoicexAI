{
  "uuid": "d811043e-7a30-43b6-b7f6-24cf511dce0c",
  "atom_type": "PolicySection",
  "name": "Page 18",
  "content": {
    "title": "Page 18",
    "text": "One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\nsize beyond N\u0003without qualitatively different data requirements, perhaps this means that once we reach\nC\u0003\nminandN\u0003, we have extracted all of the reliable information available in natural language data. In this\ninterpretation, L\u0003would provide a rough estimate for the entropy-per-token7of natural language. In this\nscenario, we would expect the loss trend to level off at or before L\u0003.\nWe can guess at the functional form of L(Cmin)as it levels off by considering a version of our training\ndataset with added noise. For example, we could append a random string of tokens to each context shown\nto the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise\nﬂoorL\u0000Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\npotentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L\u0003), and\nmay be meaningful even if it occurs after the leveling off.\n7 Related Work\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\nnumber of relevant features in the data.\nSome early [BB01, Goo01] work found power-law scalings between performance and dataset size. More\nrecent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is\nperhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of\ndataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our\nﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets\n[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent\nwork [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an\nansatz similar to ours.\nEfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal\nperformance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that\nfor language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).\nBut more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the\noverall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles\nof shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width\nand depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies\nﬁx computation per data example, which tends to scale in proportion to the number of model parameters,\nwhereas we investigate scaling with both model size and the quantity of training computation.\nVarious works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-\ning a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training\nmany orders of magnitude beyond typical practice, and in particular does not use early stopping). We do\nnot observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\nHessian spectrum [Pap18, GKX19, GARD18].\n8 Discussion\nWe have observed consistent scalings of language model log-likelihood loss with non-embedding parameter\ncountN, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and\n(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.\nSince scalings with N;D;C minare power-laws, there are diminishing returns with increasing scale.\n7Deﬁning words using the wcutility, the WebText2 dataset has 1:4tokens per word and 4:3characters per token.\n8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of\nloss on both model and dataset size.\n18",
    "page": 18
  },
  "metadata": {
    "source": "docs/2001.08361.pdf"
  },
  "attention_value": {
    "short_term": 0.2,
    "long_term": 0.4757998708764747,
    "very_long_term": 0.0
  },
  "truth_value": {
    "strength": 1.0,
    "confidence": 1.0
  },
  "incoming_links": [],
  "outgoing_links": [],
  "created_at": "2025-10-21T01:56:01.900447",
  "updated_at": "2025-10-21T02:01:01.030051",
  "last_accessed": "2025-10-21T01:56:01.903570"
}