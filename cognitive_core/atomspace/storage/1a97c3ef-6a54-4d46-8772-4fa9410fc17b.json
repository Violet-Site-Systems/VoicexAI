{
  "uuid": "1a97c3ef-6a54-4d46-8772-4fa9410fc17b",
  "atom_type": "PolicySection",
  "name": "Page 29",
  "content": {
    "title": "Page 29",
    "text": "[HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le,\nand Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\nCoRR , abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965 . 19\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\nninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\ndictable, empirically, 2017, 1712.00409. 18\n[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in neural information processing systems , pages\n8571–8580, 2018. 18\n[KB14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014,\n1412.6980. 7\n[Kom19] Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18\n[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\nInformation Processing Systems - Volume 1 , NIPS’12, pages 1097–1105, USA, 2012. Curran\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257 . 19\n[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations, 2019,\n1909.11942. 9\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-\ning approach. CoRR , abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/\n1907.11692 . 2\n[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs] ,\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198 . 2, 6\n[LT16] Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv\npreprint arXiv:1606.06737 , 2016. 25\n[LXS+19] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent, 2019, arXiv:1902.06720. 18\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\n[Pap18] Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\nCoRR , abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062 . 18\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/languageunsupervised/language understanding paper. pdf , 2018. 2, 6\n[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\nprediction of the generalization error across scales, 2019, 1909.12673. 18\n[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\nprediction of the generalization error across scales, 2019, arXiv:1909.12673. 18\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer, 2019, arXiv:1910.10683. 2\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. openai.com , 2019. 2, 5, 6, 7, 8\n[SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\nBlake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. CoRR , 2015, 1508.07909. 6\n29",
    "page": 29
  },
  "metadata": {
    "source": "docs/2001.08361.pdf"
  },
  "attention_value": {
    "short_term": 0.2,
    "long_term": 0.4757998708764747,
    "very_long_term": 0.0
  },
  "truth_value": {
    "strength": 1.0,
    "confidence": 1.0
  },
  "incoming_links": [],
  "outgoing_links": [],
  "created_at": "2025-10-21T01:56:01.903107",
  "updated_at": "2025-10-21T02:01:01.032814",
  "last_accessed": "2025-10-21T01:56:01.903581"
}