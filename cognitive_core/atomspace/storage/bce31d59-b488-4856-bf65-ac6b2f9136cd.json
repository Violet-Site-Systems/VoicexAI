{
  "uuid": "bce31d59-b488-4856-bf65-ac6b2f9136cd",
  "atom_type": "PolicySection",
  "name": "Page 19",
  "content": {
    "title": "Page 19",
    "text": "We were able to precisely model the dependence of the loss on NandD, and alternatively on NandS, when\nthese parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\nof overﬁtting, early stopping step, and data requirements when training large language models. So our scaling\nrelations go beyond mere observation to provide a predictive framework. One might interpret these relations\nas analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,\nindependent of most of the details of its microscopic consituents.\nIt is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a\nmaximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to\ntest these relations on other domains, such as images, audio, and video models, and perhaps also for random\nnetwork distillation. At this point we do not know which of our results depend on the structure of natural\nlanguage data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from\nwhich the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we\nhave observed. Such a theory might make it possible to derive other more precise predictions, and provide a\nsystematic understanding of the limitations of the scaling laws.\nIn the domain of natural language, it will be important to investigate whether continued improvement on the\nloss translates into improvement on relevant language tasks. Smooth quantitative change can mask major\nqualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy\nprovides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth\nimprovements in language model loss may hide seemingly qualitative changes in capability.\nOur results strongly suggest that larger models will continue to perform better, and will also be much more\nsample efﬁcient than has been previously appreciated. Big models may be more important than big data.\nIn this context, further investigation into model parallelism is warranted. Deep models can be trained using\npipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased\nbatch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization\n[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity\n[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through\nincreased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,\nit might be possible to remain on the compute-efﬁcient frontier for an entire training run.\nAcknowledgements\nWe would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\nback on drafts of this work.\n19",
    "page": 19
  },
  "metadata": {
    "source": "docs/2001.08361.pdf"
  },
  "attention_value": {
    "short_term": 0.2,
    "long_term": 0.4335948456850463,
    "very_long_term": 0.0
  },
  "truth_value": {
    "strength": 1.0,
    "confidence": 1.0
  },
  "incoming_links": [],
  "outgoing_links": [],
  "created_at": "2025-10-21T01:56:34.782795",
  "updated_at": "2025-10-21T02:01:01.076612",
  "last_accessed": "2025-10-21T01:56:34.791430"
}