{
  "uuid": "b387a188-e87b-4d21-9b4d-5055df58850d",
  "atom_type": "PolicySection",
  "name": "Page 6",
  "content": {
    "title": "Page 6",
    "text": "be maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total\ncompute as a power law (see Equation (1.3)).\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their\nimplications for training time, and a breakdown of our results per token. We also make some brief compar-\nisons to LSTMs and recurrent Transformers [DGV+18].\n1.3 Notation\nWe use the following notation:\n\u000fL– the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in\nsome cases we report the loss for speciﬁc tokens within the context.\n\u000fN– the number of model parameters, excluding all vocabulary and positional embeddings\n\u000fC\u00196NBS – an estimate of the total non-embedding training compute, where Bis the batch size,\nandSis the number of training steps (ie parameter updates). We quote numerical values in PF-days,\nwhere one PF-day = 1015\u000224\u00023600 = 8:64\u00021019ﬂoating point operations.\n\u000fD– the dataset size in tokens\n\u000fBcrit– the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the\ncritical batch size provides a roughly optimal compromise between time and compute efﬁciency.\n\u000fCmin– an estimate of the minimum amount of non-embedding compute to reach a given value of\nthe loss. This is the training compute that would be used if the model were trained at a batch size\nmuch less than the critical batch size.\n\u000fSmin– an estimate of the minimal number of training steps needed to reach a given value of the loss.\nThis is also the number of training steps that would be used if the model were trained at a batch size\nmuch greater than the critical batch size.\n\u000f\u000bX– power-law exponents for the scaling of the loss as L(X)/1=X\u000bXwhereXcan be any of\nN;D;C;S;B;Cmin.\n2 Background and Methods\nWe train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized\nusing byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257 . We optimize the autoregres-\nsive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal\nperformance metric. We record the loss on the WebText2 test distribution and on a selection of other text\ndistributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though\nwe also train LSTM models and Universal Transformers [DGV+18] for comparison.\n2.1 Parameter and Compute Scaling of Transformers\nWe parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-\nmension of the residual stream), d\u000b(dimension of the intermediate feed-forward layer), dattn(dimension of\nthe attention output), and nheads (number of attention heads per layer). We include nctxtokens in the input\ncontext, with nctx= 1024 except where otherwise noted.\nWe useNto denote the model size, which we deﬁne as the number of non-embedding parameters\nN\u00192dmodelnlayer(2dattn+d\u000b)\n= 12nlayerd2\nmodel with the standard dattn=d\u000b=4 =dmodel (2.1)\nwhere we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters\nin an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include\nthese when discussing the ‘model size’ N; we will see that this produces signiﬁcantly cleaner scaling laws.\nEvaluating a forward pass of the Transformer involves roughly\nCforward\u00192N+ 2nlayernctxdmodel (2.2)\nadd-multiply operations, where the factor of two comes from the multiply-accumulate operation used in\nmatrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.\n6",
    "page": 6
  },
  "metadata": {
    "source": "docs/2001.08361.pdf"
  },
  "attention_value": {
    "short_term": 0.2,
    "long_term": 0.4911807109608145,
    "very_long_term": 0.0
  },
  "truth_value": {
    "strength": 1.0,
    "confidence": 1.0
  },
  "incoming_links": [],
  "outgoing_links": [],
  "created_at": "2025-10-21T01:55:40.439800",
  "updated_at": "2025-10-21T02:01:01.034362",
  "last_accessed": "2025-10-21T01:55:40.446146"
}