{
  "uuid": "01a20597-36f0-4e68-91e9-d0755ad3deab",
  "atom_type": "PolicySection",
  "name": "Page 9",
  "content": {
    "title": "Page 9",
    "text": "LSTM plateaus after <100 tokens\nTransformer improves through the whole context\n2M200M3M300M54326\nToken Index in Context103102101Transformers asymptotically outperform LSTMs due to improved use of long contexts\n3.64.23.02.44.85.4\n105108106107109Parameters (non-embedding)TransformersLSTMs1 Layer2 Layers4 LayersTest Loss\nPer-token Test LossParameters:400K400KFigure 7\nTo observe these trends it is crucial to study performance as a function of N; if we instead use the total\nparameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This\nsuggests that the embedding matrix can be made smaller without impacting performance, as has been seen in\nrecent work [LCG+19].\nAlthough these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets\nis also a power-law in Nwith nearly identical power, as shown in Figure 8.\n3.2.1 Comparing to LSTMs and Universal Transformers\nIn Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter\ncountN. The LSTMs were trained with the same dataset and context length. We see from these ﬁgures\nthat the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match\nthe Transformer performance for later tokens. We present power-law relationships between performance and\ncontext position Appendix D.5, where increasingly large powers for larger models suggest improved ability\nto quickly recognize patterns.\nWe also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure\n17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the\ncost of additional compute per-parameter.\n3.2.2 Generalization Among Data Distributions\nWe have also tested our models on a set of additional text data distributions. The test loss on these datasets\nas a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2\ndataset. We see that the loss on these other data distributions improves smoothly with model size, in direct\nparallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the\nin-distribution validation loss, and does not depend on the duration of training or proximity to convergence.\nWe also observe no dependence on model depth (see Appendix D.8).\n3.3 Performance with Dataset Size and Compute\nWe display empirical trends for the test loss as a function of dataset size D(in tokens) and training compute\nCin Figure 1.\nFor the trend with Dwe trained a model with (nlayer;nembd) = (36;1280) on ﬁxed subsets of the WebText2\ndataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\nﬁt with simple power-law\nL(D)\u0019\u0012Dc\nD\u0013\u000bD\n(3.2)\nin the dataset size. The data and ﬁt appear in Figure 1.\nThe total amount of non-embedding compute used during training can be estimated as C= 6NBS , where\nBis the batch size, Sis the number of parameter updates, and the factor of 6accounts for the forward and\nbackward passes. Thus for a given value of Cwe can scan over all models with various Nto ﬁnd the model\n9",
    "page": 9
  },
  "metadata": {
    "source": "docs/2001.08361.pdf"
  },
  "attention_value": {
    "short_term": 0.2,
    "long_term": 0.4757998708764747,
    "very_long_term": 0.0
  },
  "truth_value": {
    "strength": 1.0,
    "confidence": 1.0
  },
  "incoming_links": [],
  "outgoing_links": [],
  "created_at": "2025-10-21T01:56:01.897969",
  "updated_at": "2025-10-21T02:01:00.998528",
  "last_accessed": "2025-10-21T01:56:01.903560"
}